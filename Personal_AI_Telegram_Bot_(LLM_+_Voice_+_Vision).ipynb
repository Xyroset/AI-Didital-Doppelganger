{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xyroset/AI-Didital-Doppelganger/blob/main/Personal_AI_Telegram_Bot_(LLM_%2B_Voice_%2B_Vision).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ðŸ¤– Personal AI Telegram Bot (LLM + Voice + Vision)**\n",
        "\n",
        "### **!! The bot is designed for a single user!!**\n",
        "Otherwise, Google Colab may freeze or overload.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Welcome!**\n",
        " This notebook allows you to deploy your own fully customizable AI companion directly in Telegram.\n",
        "\n",
        "It uses a local **LLM(unsloth)** for smart conversations, **XTTS** for natural voice generation, and the **Groq API** to instantly understand your voice messages and photos.\n",
        "\n",
        "### **Before you start:**\n",
        "\n",
        "1. **Models:** Make sure you have uploaded your LLM and TTS models to your Google Drive.\n",
        "\n",
        "2. **Hardware:** This code requires a GPU. Go to the top menu: `Runtime` => `Change runtime` type and select T4 GPU.\n",
        "\n",
        "3. **API Keys:** You will need a Telegram Bot Token (from `@BotFather`) and a free [Groq API Key](https://console.groq.com/keys \"Groq\").\n",
        "\n",
        "Just follow the steps below, run the cells one by one, and your AI friend will be online!"
      ],
      "metadata": {
        "id": "yzJLHtlQzFnK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. **ðŸ“¦ Install libraries**\n",
        "\n",
        "This is the first step. Run this block to download and install all the necessary dependencies for the AI models, voice generation (TTS), and the Telegram bot framework.\n",
        "\n",
        "After installation, you will see `Compete!` and the session will **restart**."
      ],
      "metadata": {
        "id": "rWtqaX-GacA_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9PfX83rDHxF-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96ea2b53-2b37-44b9-a798-128f50245855"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[1/5] Install system libraries..\n",
            "\n",
            "[2/5] Set default state for Unsloth & Bot...\n",
            "   => Installing Unsloth...\n",
            "   => Installing Aiogram, Groq and etc\n",
            "\n",
            "[3/5] Create venv for TTS...\n",
            "\n",
            "[4/5] Install libraries for VENV (Torch, TTS)...\n",
            "   => Fix version inside venv...\n",
            "\n",
            "[5/5] Confirm patch...\n",
            "\n",
            "Complete! 529 sec.\n",
            "Restart...\n"
          ]
        }
      ],
      "source": [
        "# @title ### **Install**\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "venv_dir = \"/content/tts_venv\"\n",
        "venv_bin = f\"{venv_dir}/bin\"\n",
        "venv_python = f\"{venv_bin}/python\"\n",
        "venv_pip = f\"{venv_bin}/pip\"\n",
        "\n",
        "print(\"\\n[1/5] Install system libraries..\")\n",
        "subprocess.run(\"sudo apt-get update -qq\", shell=True)\n",
        "subprocess.run(\"sudo apt-get install -y -qq espeak-ng libsndfile1-dev ffmpeg\", shell=True)\n",
        "\n",
        "print(\"\\n[2/5] Set default state for Unsloth & Bot...\")\n",
        "\n",
        "subprocess.run(\"pip uninstall -y TTS coqui-tts transformers tokenizers numpy\", shell=True)\n",
        "\n",
        "print(\"   => Installing Unsloth...\")\n",
        "subprocess.run(\"pip install 'unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git'\", shell=True)\n",
        "subprocess.run(\"pip install --no-deps 'xformers<0.0.27' 'trl<0.8.0' peft accelerate bitsandbytes\", shell=True)\n",
        "\n",
        "print(\"   => Installing Aiogram, Groq and etc\")\n",
        "subprocess.run(\"pip install aiogram groq Pillow moviepy lottie cairosvg\", shell=True)\n",
        "\n",
        "print(\"\\n[3/5] Create venv for TTS...\")\n",
        "\n",
        "if not os.path.exists(venv_python):\n",
        "    print(\"   => Create folder venv...\")\n",
        "    subprocess.run(f\"python3 -m venv {venv_dir}\", shell=True)\n",
        "\n",
        "if not os.path.exists(venv_pip):\n",
        "    subprocess.run(\"curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\", shell=True)\n",
        "    subprocess.run(f\"{venv_python} get-pip.py\", shell=True)\n",
        "\n",
        "print(\"\\n[4/5] Install libraries for VENV (Torch, TTS)...\")\n",
        "\n",
        "def install_in_venv(args):\n",
        "    cmd = f\"{venv_pip} install {args}\"\n",
        "    try:\n",
        "        subprocess.check_call(cmd.split())\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(f\"Erro: {args}\")\n",
        "        raise\n",
        "try:\n",
        "    install_in_venv(\"--upgrade pip\")\n",
        "\n",
        "    install_in_venv(\"torch torchaudio --index-url https://download.pytorch.org/whl/cu121\")\n",
        "\n",
        "    install_in_venv(\"coqui-tts==0.24.1\")\n",
        "    install_in_venv(\"torchcodec soundfile typing-extensions\")\n",
        "\n",
        "    print(\"   => Fix version inside venv...\")\n",
        "    install_in_venv(\"numpy==1.26.4 transformers==4.45.2 tokenizers==0.20.3\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error Venv: {e}\")\n",
        "\n",
        "print(\"\\n[5/5] Confirm patch...\")\n",
        "\n",
        "target_file = f\"{venv_dir}/lib/python3.12/site-packages/coqpit/coqpit.py\"\n",
        "\n",
        "if not os.path.exists(target_file):\n",
        "    try:\n",
        "        found = subprocess.check_output(f\"find {venv_dir} -name coqpit.py\", shell=True, text=True).strip()\n",
        "        if found: target_file = found\n",
        "    except: pass\n",
        "\n",
        "if os.path.exists(target_file):\n",
        "    with open(target_file, \"r\") as f:\n",
        "        content = f.read()\n",
        "\n",
        "    patched = False\n",
        "\n",
        "    if \"if issubclass(field_type, Serializable):\" in content:\n",
        "        content = content.replace(\n",
        "            \"if issubclass(field_type, Serializable):\",\n",
        "            \"if isinstance(field_type, type) and issubclass(field_type, Serializable):\"\n",
        "        )\n",
        "        patched = True\n",
        "\n",
        "    if 'raise ValueError(f\" [!] \\'{type(x)}\\' value type' in content:\n",
        "         content = content.replace('raise ValueError(f\" [!] \\'{type(x)}\\' value type', 'pass # SUPPRESSED')\n",
        "         patched = True\n",
        "\n",
        "    if patched:\n",
        "        with open(target_file, \"w\") as f:\n",
        "            f.write(content)\n",
        "        print(\"   => Patch is complete.\")\n",
        "else:\n",
        "    print(\"   => File coqpit.py is not excist\")\n",
        "\n",
        "elapsed = int(time.time() - start_time)\n",
        "print(f\"\\nComplete! {elapsed} sec.\")\n",
        "print(\"Restart...\")\n",
        "\n",
        "time.sleep(3)\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. âš™ Settings**\n",
        "\n",
        "### **Secret keys:**\n",
        "\n",
        "1. Look at the left sidebar of Google Colab and click on the ðŸ”‘ (Key icon) named ***\"Secrets\"***.\n",
        "\n",
        "2. **Telegram Token:** Go to Telegram, message `@BotFather`, use the `/newbot` command, and copy your **HTTP API Token**. Create a new secret in Colab named exactly `BOT_API` and paste your token as the value.\n",
        "\n",
        "3. **Groq API Key:** Go to `console.groq.com`, sign in, and generate a new API key. Create a second secret in Colab named exactly `GROQ_API` and paste the key.\n",
        "\n",
        "4. **Crucial Step:** Toggle the ***\"Notebook access\"*** switch to ON for both secrets!\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Basic Settings:**\n",
        "\n",
        "1. **BOT_NAME & LANGUAGE:** Choose a name for your AI and select the primary language for voice generation.\n",
        "\n",
        "2. **Model Paths:** Ensure these match the exact folder paths on your Google Drive where the LLM(text) and TTS(voice) models are stored.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **LLM Loading Parameters Guide**\n",
        "\n",
        "Unsloth models, you might need to adjust the loading parameters to avoid Out Of Memory (OOM) errors on a T4 GPU (15GB VRAM).\n",
        "\n",
        "* **Max Sequence Length (`max_seq_length`):** The \"memory window\" of the AI. Higher values let the bot remember longer chats, but consume more VRAM.\n",
        "    * `2048` - Safe mode (fast, minimal VRAM).\n",
        "    * `4096` - Standard for roleplay and normal chats.\n",
        "    * `8192` - Maximum recommended for T4 GPU with 8B models.\n",
        "* **Load in 4-bit (`load_in_4bit`):** Quantization. **Must be TRUE** for 7B-9B models on a T4 GPU. You can uncheck it (False) only if you use tiny models (1.5B - 3B).\n",
        "* **Dtype:** Data type for weights. Leave it at `None` to let Unsloth auto-detect the best format (usually float16 for Colab).\n",
        "\n",
        "**Popular Unsloth Models (GGUF/Safetensors) & Settings for T4 GPU:**\n",
        "1.  `unsloth/Llama-3.1-8B-bnb-4bit`: Seq=2048, 4-bit=True, Dtype=None\n",
        "2.  `unsloth/Qwen3-4B-Base`: Seq=2048, 4-bit=True, Dtype=None\n",
        "3.  `unsloth/gpt-oss-20b-unsloth-bnb-4bit`: Seq=1024, 4-bit=True, Dtype=None\n",
        "4.  `unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit`: Seq=4096, 4-bit=True, Dtype=None\n",
        "5.  `unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit` (12B): Seq=2048, 4-bit=True, Dtype=None\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Advanced Settings (LLM & TTS):**\n",
        "\n",
        "1. **Temperature:** Controls creativity. Lower values make the AI logical and strict; higher values make it more creative and unpredictable.\n",
        "\n",
        "2. **Max Tokens:** Limits the maximum length of the bot's text responses.\n",
        "\n",
        "3. **Repetition Penalty & Top K/P:** Advanced parameters that prevent the AI from looping or repeating words, controlling its vocabulary richness. If unsure, leave them at their default values!\n",
        "\n",
        "**Action:** After setting up your secrets and sliders, run all cells in this section to save your configurations.\n"
      ],
      "metadata": {
        "id": "GQ2WuxuGRVJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## **Base**\n",
        "\n",
        "# @markdown ### **Bot**\n",
        "BOT_NAME = \"AI Assistant\" # @param {type:\"string\"}\n",
        "LANGUAGE = \"en\" # @param [\"en\", \"ru\", \"es\", \"fr\", \"de\", \"ja\"]\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown <br>\n",
        "\n",
        "# @markdown ### **API**\n",
        "SECRET_BOT_API = \"BOT_API\" # @param {type:\"string\"}\n",
        "SECRET_GROQ_API = \"GROQ_API\"  # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown <br>\n",
        "\n",
        "# @markdown ### **Model Path**\n",
        "LLM_MODEL_PATH = \"/content/drive/My Drive/LLM_Model\" # @param {type:\"string\"}\n",
        "TTS_MODEL_PATH = \"/content/drive/My Drive/TTS_Model\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown <br>\n",
        "\n",
        "# @markdown ### **LLM Loading (Unsloth)**\n",
        "LLM_MAX_SEQ_LENGTH = 2048 # @param {type:\"slider\", min:1024, max:16384, step:1024}\n",
        "LLM_LOAD_IN_4BIT = True # @param {type:\"boolean\"}\n",
        "LLM_DTYPE = \"None\" # @param [\"None\", \"float16\", \"bfloat16\"]\n",
        "\n",
        "print(\"Settings confirmed!\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "GOtYPIxIFec4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08d1c339-782e-4e2f-e125-058232609e97"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Settings confirmed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### **Advanced**\n",
        "\n",
        "# @markdown ### **LLM**\n",
        "LLM_TEMPERATURE = 0.6 # @param {type:\"slider\", min:0.1, max:1.0, step:0.1}\n",
        "MAX_TOKENS = 128 # @param {type:\"slider\", min:64, max:1024, step:64}\n",
        "LLM_REPETITION_PENALTY = 1.1 # @param {type:\"slider\", min:1.0, max:2.0, step:0.05}\n",
        "LLM_TOP_K = 50 # @param {type:\"slider\", min:10, max:100, step:5}\n",
        "LLM_TOP_P = 0.95 # @param {type:\"slider\", min:0.5, max:1.0, step:0.05}\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown <br>\n",
        "\n",
        "# @markdown ### **TTS**\n",
        "TTS_TEMPERATURE = 0.65 # @param {type:\"slider\", min:0.1, max:1.0, step:0.05}\n",
        "TTS_REPETITION_PENALTY = 2 # @param {type:\"slider\", min:1.0, max:2.0, step:0.05}\n",
        "TTS_TOP_K = 40 # @param {type:\"slider\", min:10, max:100, step:5}\n",
        "TTS_TOP_P = 0.8 # @param {type:\"slider\", min:0.5, max:1.0, step:0.05}\n",
        "\n",
        "print(\"Advanced Settings confirmed!\")"
      ],
      "metadata": {
        "id": "UJQ-r07InUJF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfca3ee1-96e4-4c24-c094-f9f03464674a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Advanced Settings confirmed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. ðŸ—ï¸ Load models**\n",
        "\n",
        "#### **!! If you have changed the settings, then restart this block !!**\n",
        "\n",
        "Press **Run and wait**. Do not proceed until you see the `Complete!` message at the bottom. It usually takes a few minutes."
      ],
      "metadata": {
        "id": "iXe-Va5MTb69"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "pV5wdTOkID7F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cab7ffb4-eb3c-4f2b-9cf2-905cd9b4ee58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Cleaning up old background processes...\n",
            "==((====))==  Unsloth 2026.2.1: Fast Llama patching. Transformers: 4.57.6.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.563 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = None. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth 2026.2.1 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Complete! 54 sec.\n"
          ]
        }
      ],
      "source": [
        "# @title ### **Load**\n",
        "\n",
        "import time\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"Cleaning up old background processes...\")\n",
        "os.system(\"pkill -f tts_runner.py\")\n",
        "os.system(\"fuser -k 5050/tcp\")\n",
        "time.sleep(2)\n",
        "\n",
        "\n",
        "parsed_dtype = None\n",
        "if LLM_DTYPE == \"float16\":\n",
        "    parsed_dtype = torch.float16\n",
        "elif LLM_DTYPE == \"bfloat16\":\n",
        "    parsed_dtype = torch.bfloat16\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = LLM_MODEL_PATH,\n",
        "    max_seq_length = LLM_MAX_SEQ_LENGTH,\n",
        "    dtype = parsed_dtype,\n",
        "    load_in_4bit = LLM_LOAD_IN_4BIT,\n",
        ")\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "runner_code = \"\"\"\n",
        "import sys\n",
        "import os\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "os.environ[\"MPLBACKEND\"] = \"Agg\"\n",
        "\n",
        "import traceback\n",
        "import json\n",
        "import subprocess\n",
        "import torch\n",
        "import torchaudio\n",
        "from http.server import BaseHTTPRequestHandler, HTTPServer\n",
        "from TTS.tts.configs.xtts_config import XttsConfig\n",
        "from TTS.tts.models.xtts import Xtts\n",
        "\n",
        "torch.set_num_threads(4)\n",
        "\n",
        "MODEL_PATH = \"[[TTS_MODEL_PATH]]\"\n",
        "REF_AUDIO = f\"{MODEL_PATH}/reference.wav\"\n",
        "\n",
        "config = XttsConfig()\n",
        "config.load_json(f\"{MODEL_PATH}/config.json\")\n",
        "tts_model = Xtts.init_from_config(config)\n",
        "tts_model.load_checkpoint(config, checkpoint_dir=MODEL_PATH, use_deepspeed=False)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    tts_model.cuda()\n",
        "\n",
        "class TTSHandler(BaseHTTPRequestHandler):\n",
        "    def do_POST(self):\n",
        "        try:\n",
        "            content_length = int(self.headers['Content-Length'])\n",
        "            data = json.loads(self.rfile.read(content_length))\n",
        "\n",
        "            text = data.get('text', '').replace('\\\\n', ' ').replace('\\\\r', ' ').strip()\n",
        "            if not text:\n",
        "                text = \"Empty text\"\n",
        "\n",
        "            output_file = data.get('output_file', 'response.ogg')\n",
        "\n",
        "            out = tts_model.synthesize(\n",
        "                text, config, speaker_wav=REF_AUDIO, gpt_cond_len=3,\n",
        "                language=\"[[LANGUAGE]]\",\n",
        "                temperature=float([[TTS_TEMPERATURE]]),\n",
        "                repetition_penalty=float([[TTS_REPETITION_PENALTY]]),\n",
        "                top_k=int([[TTS_TOP_K]]),\n",
        "                top_p=float([[TTS_TOP_P]])\n",
        "            )\n",
        "\n",
        "            temp_wav = \"temp_raw.wav\"\n",
        "            torchaudio.save(temp_wav, torch.tensor(out[\"wav\"]).unsqueeze(0), 24000)\n",
        "\n",
        "            subprocess.run([\n",
        "                \"ffmpeg\", \"-y\", \"-i\", temp_wav,\n",
        "                \"-c:a\", \"libopus\", \"-b:a\", \"32k\", \"-vbr\", \"on\", output_file\n",
        "            ], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "\n",
        "            if os.path.exists(temp_wav):\n",
        "                os.remove(temp_wav)\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            self.send_response(200)\n",
        "            self.send_header('Content-type', 'application/json')\n",
        "            self.end_headers()\n",
        "            self.wfile.write(json.dumps({\"status\": \"success\", \"file\": output_file}).encode('utf-8'))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\\\n[x] Error generation:\")\n",
        "            traceback.print_exc()\n",
        "            self.send_response(500)\n",
        "            self.end_headers()\n",
        "\n",
        "    def log_message(self, format, *args):\n",
        "        pass\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    server = HTTPServer(('127.0.0.1', 5050), TTSHandler)\n",
        "    server.serve_forever()\n",
        "\"\"\"\n",
        "\n",
        "runner_code = runner_code.replace(\"[[TTS_MODEL_PATH]]\", TTS_MODEL_PATH)\n",
        "runner_code = runner_code.replace(\"[[LANGUAGE]]\", LANGUAGE)\n",
        "runner_code = runner_code.replace(\"[[TTS_TEMPERATURE]]\", str(TTS_TEMPERATURE))\n",
        "runner_code = runner_code.replace(\"[[TTS_REPETITION_PENALTY]]\", str(TTS_REPETITION_PENALTY))\n",
        "runner_code = runner_code.replace(\"[[TTS_TOP_K]]\", str(TTS_TOP_K))\n",
        "runner_code = runner_code.replace(\"[[TTS_TOP_P]]\", str(TTS_TOP_P))\n",
        "\n",
        "with open(\"tts_runner.py\", \"w\") as f:\n",
        "    f.write(runner_code)\n",
        "\n",
        "my_env = os.environ.copy()\n",
        "my_env[\"MPLBACKEND\"] = \"Agg\"\n",
        "\n",
        "subprocess.Popen(\n",
        "    \"nohup /content/tts_venv/bin/python -u tts_runner.py > tts_server.log 2>&1 &\",\n",
        "    shell=True,\n",
        "    env=my_env,\n",
        "    preexec_fn=os.setpgrp\n",
        ")\n",
        "time.sleep(12)\n",
        "\n",
        "elapsed = int(time.time() - start_time)\n",
        "print(f\"\\nComplete! {elapsed} sec.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. ðŸŽ¨ Personalization**\n",
        "\n",
        "This block allows you to completely customize your bot's personality and interface texts without touching the main code!\n",
        "\n",
        "**Base:** Define your bot's core identity in the `SYSTEM_INSTRUCTION` (e.g., \"You are **Michael**, a sarcastic coding assistant\") and adjust the `TIMER_DEBOUNCE` (how many seconds the bot waits for you to finish typing before generating a response).\n",
        "\n",
        "**Commands:** Customize the descriptions for standard Telegram commands (`/start`, `/reset`, etc.) and change the exact phrases the bot uses to reply to them.\n",
        "\n",
        "**Temp message:** Translate or change the temporary status messages (like ***\"Typing...\"*** or ***\"Generating voice...\"***) to fit your bot's vibe.\n",
        "\n",
        "**Action:** Adjust the text fields to your liking and press the ***\"Run\"*** button to save your personalized settings before starting the bot."
      ],
      "metadata": {
        "id": "SjIOTeB2cWmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### **Settings**\n",
        "\n",
        "# @markdown ### **Base**\n",
        "SYSTEM_INSTRUCTION = \"Your task is to answer the questions in your own style. If you see a description of an image, sticker, or animation, answer it in your own style.\" # @param {type:\"string\"}\n",
        "TIMER_DEBOUNCE = 4 # @param {type:\"slider\", min:1, max:30, step:1}\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown <br>\n",
        "\n",
        "# @markdown ### **Command Start**\n",
        "START_DESCRIPTION = \"Start chatting with the bot\" # @param {type:\"string\"}\n",
        "WELLCOME_MESSAGE = f\"Hello! I am {BOT_NAME}. How can I help you today?\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown <br>\n",
        "\n",
        "# @markdown ### **Commmand Reset**\n",
        "RESET_DESCRIPTION = \"Regenerate the last message\" # @param {type:\"string\"}\n",
        "THERE_ARE_NO_MESSAGE = \"There is no previous message to reset.\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown <br>\n",
        "\n",
        "# @markdown ### **Commmand Reset Memory**\n",
        "RESET_MEMORY_DESCRIPTION = \"Clear chat history and context\" # @param {type:\"string\"}\n",
        "CLEANING_MESSAGE = \"Memory cleared! Let's start a new conversation.\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown <br>\n",
        "\n",
        "# @markdown ### **Commmand Voice Mode**\n",
        "VOICE_MODE_DESCRIPTION = \"Toggle text-to-speech voice replies\" # @param {type:\"string\"}\n",
        "OFF_VOICE_MODE_MESSAGE = \"Voice mode disabled. I will reply with text.\" # @param {type:\"string\"}\n",
        "ON_VOICE_MODE_MESSAGE = \"Voice mode enabled. I will now reply with audio messages.\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown <br>\n",
        "\n",
        "# @markdown ### **Temp message**\n",
        "TEXT_TEMP_MESSAGE = \"Typing...\" # @param {type:\"string\"}\n",
        "VOICE_TEMP_MESSAGE = \"Generating voice...\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown <br>\n",
        "\n",
        "print(\"Personalization confirmed!\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "LBieiBQ1RYOr",
        "outputId": "2d54e706-0d0c-4cfe-8085-084fb9cc8cb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Personalization confirmed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. ðŸš€ Start telegram bot**\n",
        "**The final step!** This block contains the main logic and keeps your bot online.\n",
        "\n",
        "**Action:** Run this cell. Once you see the `Bot is running!` message in the console, open Telegram, find your bot, and send the `/start command`.\n",
        "\n",
        "**Important:** Keep this cell running and the browser tab open while you are chatting with your AI. If the execution stops, the bot will go offline."
      ],
      "metadata": {
        "id": "-BupHrRXcr98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# @title ### **Start**\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "import asyncio, os, re, subprocess, base64, requests, gc\n",
        "from typing import Callable, Dict, Any, Awaitable\n",
        "\n",
        "from aiogram import Bot, Dispatcher, types, F, BaseMiddleware\n",
        "from aiogram.types import BotCommand, BotCommandScopeDefault, FSInputFile\n",
        "from aiogram.utils.chat_action import ChatActionSender\n",
        "from aiogram.filters import Command\n",
        "\n",
        "from groq import Groq\n",
        "from PIL import Image\n",
        "from collections import deque\n",
        "from moviepy.editor import VideoFileClip\n",
        "from lottie.parsers.tgs import parse_tgs\n",
        "from lottie.exporters.gif import export_gif\n",
        "\n",
        "# Initialize clients (Make sure keys are stored in Colab Secrets)\n",
        "groq_client = Groq(api_key=userdata.get(SECRET_GROQ_API))\n",
        "bot = Bot(token=userdata.get(SECRET_BOT_API))\n",
        "dp = Dispatcher()\n",
        "\n",
        "MAX_HISTORY_TOKENS = LLM_MAX_SEQ_LENGTH - MAX_TOKENS - 300\n",
        "\n",
        "if MAX_HISTORY_TOKENS < 500:\n",
        "    MAX_HISTORY_TOKENS = 500\n",
        "\n",
        "user_histories = {}\n",
        "voice_mode = {}\n",
        "user_messages = {}\n",
        "user_timers = {}\n",
        "last_message_obj = {}\n",
        "last_user_text = {}\n",
        "\n",
        "llm_generation_lock = asyncio.Lock()\n",
        "tts_generation_lock = asyncio.Lock()\n",
        "\n",
        "# Voice TTS Engine\n",
        "class VoiceEngine:\n",
        "    def __init__(self):\n",
        "        self.api_url = \"http://127.0.0.1:5050\"\n",
        "\n",
        "    def text_to_audio(self, text, output_filename=\"response.ogg\"):\n",
        "        try:\n",
        "            payload = {\"text\": text, \"output_file\": output_filename}\n",
        "            # Send text\n",
        "            response = requests.post(self.api_url, json=payload)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                return output_filename\n",
        "            else:\n",
        "                print(\"Error: TTS Server returned a non-200 status.\")\n",
        "                return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error connecting to TTS Server: {e}\")\n",
        "            return None\n",
        "\n",
        "voice_engine = VoiceEngine()\n",
        "\n",
        "# Vision (Groq API)\n",
        "def describe_image(image_path: str) -> str:\n",
        "    try:\n",
        "        with open(image_path, \"rb\") as image_file:\n",
        "            encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "        image_url = f\"data:image/jpeg;base64,{encoded_string}\"\n",
        "\n",
        "        chat_completion = groq_client.chat.completions.create(\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\"type\": \"text\", \"text\": \"Describe in detail what is in this picture.\"},\n",
        "                        {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n",
        "                    ],\n",
        "                }\n",
        "            ],\n",
        "            model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        "        )\n",
        "        return chat_completion.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        return f\"Vision Error: {e}\"\n",
        "\n",
        "# Transcribe (Whisper API)\n",
        "def transcribe_audio(audio_path: str) -> str:\n",
        "    try:\n",
        "        with open(audio_path, \"rb\") as file:\n",
        "            transcription = groq_client.audio.transcriptions.create(\n",
        "                file=(audio_path, file.read()),\n",
        "                model=\"whisper-large-v3-turbo\",\n",
        "                language=LANGUAGE,\n",
        "                response_format=\"json\",\n",
        "                temperature=0.0\n",
        "            )\n",
        "        return transcription.text\n",
        "    except Exception as e:\n",
        "        return f\"Audio Error: {e}\"\n",
        "\n",
        "# Generate text message\n",
        "def generate_message(message: str, instruction: str) -> str:\n",
        "    input_text = alpaca_prompt.format(\n",
        "        instruction,\n",
        "        message,\n",
        "        \"\",\n",
        "    )\n",
        "\n",
        "    inputs = tokenizer([input_text], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "    input_length = inputs.input_ids.shape[1]\n",
        "    safe_length = LLM_MAX_SEQ_LENGTH - MAX_TOKENS - 50\n",
        "    if input_length > safe_length:\n",
        "        inputs = {k: v[:, -safe_length:] for k, v in inputs.items()}\n",
        "\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens = MAX_TOKENS,\n",
        "        use_cache = True,\n",
        "        temperature = LLM_TEMPERATURE,\n",
        "        repetition_penalty = LLM_REPETITION_PENALTY,\n",
        "        top_k = LLM_TOP_K,\n",
        "        top_p = LLM_TOP_P,\n",
        "        tokenizer = tokenizer,\n",
        "        stop_strings = [\"\\n###\", \"###\", \"[System\", \"User:\"],\n",
        "    )\n",
        "\n",
        "    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "    del inputs\n",
        "    del outputs\n",
        "    gc.collect()\n",
        "\n",
        "    clean_text = response.split(\"### Response:\\n\")[-1].split(\"###\")[0]\n",
        "\n",
        "    import re\n",
        "    clean_text = re.sub(r'<think>.*?</think>', '', clean_text, flags=re.DOTALL)\n",
        "\n",
        "    artifacts = [\n",
        "        \"<|end_of_text|>\", \"<|eot_id|>\", \"<ï½œendâ–ofâ–sentenceï½œ>\",\n",
        "        \"<|im_end|>\", \"</s>\", \"<eos>\"\n",
        "    ]\n",
        "    if tokenizer.eos_token:\n",
        "        artifacts.append(tokenizer.eos_token)\n",
        "\n",
        "    for artifact in artifacts:\n",
        "        clean_text = clean_text.replace(artifact, \"\")\n",
        "\n",
        "    return clean_text.strip()\n",
        "\n",
        "# Format converters for media\n",
        "def convert_webp_to_jpg(input_path: str, output_path: str):\n",
        "    with Image.open(input_path) as img:\n",
        "        rgb_im = img.convert(\"RGB\")\n",
        "        rgb_im.save(output_path, \"JPEG\", quality=95)\n",
        "\n",
        "def convert_webm_to_jpg(input_path: str, output_path: str):\n",
        "    clip = VideoFileClip(input_path)\n",
        "    clip.save_frame(output_path, t=0)\n",
        "    clip.close()\n",
        "\n",
        "    del clip\n",
        "    gc.collect()\n",
        "\n",
        "def convert_tgs_to_jpg(input_path: str, output_path: str):\n",
        "    temp_gif = \"temp_sticker.gif\"\n",
        "    animation = parse_tgs(input_path)\n",
        "    export_gif(animation, temp_gif)\n",
        "\n",
        "    try:\n",
        "        with Image.open(temp_gif) as img:\n",
        "            img.seek(0)\n",
        "            background = Image.new(\"RGB\", img.size, (255, 255, 255))\n",
        "            img_rgba = img.convert(\"RGBA\")\n",
        "            background.paste(img_rgba, mask=img_rgba.split()[3])\n",
        "            background.save(output_path, \"JPEG\", quality=95)\n",
        "    finally:\n",
        "        if os.path.exists(temp_gif):\n",
        "            os.remove(temp_gif)\n",
        "\n",
        "\n",
        "# Safe History\n",
        "def get_safe_history(user_id, new_text=\"\") -> str:\n",
        "    if user_id not in user_histories:\n",
        "        user_histories[user_id] = deque(maxlen=50)\n",
        "        return \"\"\n",
        "\n",
        "    while True:\n",
        "        history_text = \"\".join([f\"{role}: {text}\\n\" for role, text in user_histories[user_id]])\n",
        "        test_prompt = f\"{history_text}\\nUser: {new_text}\"\n",
        "\n",
        "        try:\n",
        "            tokens = tokenizer.encode(test_prompt)\n",
        "            token_count = len(tokens)\n",
        "        except Exception:\n",
        "            token_count = 0\n",
        "\n",
        "        if token_count <= MAX_HISTORY_TOKENS or len(user_histories[user_id]) == 0:\n",
        "            break\n",
        "\n",
        "        user_histories[user_id].popleft()\n",
        "\n",
        "    return history_text\n",
        "\n",
        "# Main stream handler\n",
        "async def convertor(event: types.Message, user_id: int) -> str:\n",
        "  file_path = \"\"\n",
        "  output_filename = \"\"\n",
        "\n",
        "  # Text\n",
        "  if event.text:\n",
        "    response = event.text\n",
        "\n",
        "  # Voice\n",
        "  elif event.voice:\n",
        "    file = await bot.get_file(event.voice.file_id)\n",
        "    file_path = f\"{user_id}_{file.file_id}.ogg\"\n",
        "    await bot.download(file=file, destination=file_path)\n",
        "\n",
        "    try:\n",
        "        response = transcribe_audio(file_path)\n",
        "    except Exception as e:\n",
        "        response = f\"Error: {e}\"\n",
        "\n",
        "  # Photo\n",
        "  elif event.photo:\n",
        "    file_path = f\"{user_id}.jpg\"\n",
        "    await bot.download(event.photo[-1], destination=file_path)\n",
        "\n",
        "    try:\n",
        "        response = f\"(Photo description: {describe_image(file_path)})\"\n",
        "    except Exception as e:\n",
        "        response = f\"Error: {e}\"\n",
        "\n",
        "  # Sticker\n",
        "  elif event.sticker:\n",
        "    file = await bot.get_file(event.sticker.file_id)\n",
        "    file_path = file.file_path.split(\"/\")[-1]\n",
        "\n",
        "    await bot.download(file, destination=file_path)\n",
        "    output_filename = f\"{file_path}.jpg\"\n",
        "\n",
        "    if event.sticker.is_video:\n",
        "        await asyncio.to_thread(convert_webm_to_jpg, file_path, output_filename)\n",
        "    elif event.sticker.is_animated:\n",
        "        await asyncio.to_thread(convert_tgs_to_jpg, file_path, output_filename)\n",
        "    else:\n",
        "        await asyncio.to_thread(convert_webp_to_jpg, file_path, output_filename)\n",
        "\n",
        "    try:\n",
        "        response = f\"(Sticker description: {describe_image(output_filename)})\"\n",
        "    except Exception as e:\n",
        "        response = f\"Error: {e}\"\n",
        "\n",
        "  # Animation\n",
        "  elif event.animation:\n",
        "    file = await bot.get_file(event.animation.file_id)\n",
        "    file_path = f\"{user_id}_{file.file_id}.mp4\"\n",
        "\n",
        "    await bot.download(file, destination=file_path)\n",
        "    output_filename = f\"{file_path}.jpg\"\n",
        "\n",
        "    await asyncio.to_thread(convert_webm_to_jpg, file_path, output_filename)\n",
        "\n",
        "    try:\n",
        "        response = f\"(Animation description: {describe_image(output_filename)})\"\n",
        "    except Exception as e:\n",
        "        respone = f\"Error: {e}\"\n",
        "\n",
        "\n",
        "  if os.path.exists(file_path): os.remove(file_path)\n",
        "  if os.path.exists(output_filename): os.remove(output_filename)\n",
        "\n",
        "  return response\n",
        "\n",
        "# Main Middleware for Debounce\n",
        "class MainMiddleware(BaseMiddleware):\n",
        "  async def __call__(\n",
        "      self,\n",
        "      handler: Callable[[types.Message, Dict[str, Any]], Awaitable[Any]],\n",
        "      event: types.Message,\n",
        "      data: Dict[str, Any]\n",
        ") -> Any:\n",
        "\n",
        "    # Ignor commands\n",
        "    if event.text and event.text.startswith('/'):\n",
        "      return await handler(event, data)\n",
        "\n",
        "    user_id = event.from_user.id\n",
        "\n",
        "    if user_id not in user_messages:\n",
        "      user_messages[user_id] = []\n",
        "\n",
        "    # Convert all message to text\n",
        "    new_text = await convertor(event, user_id)\n",
        "    user_messages[user_id].append(new_text)\n",
        "\n",
        "    # Cancel old task timer\n",
        "    if user_id in user_timers and not user_timers[user_id].done():\n",
        "      user_timers[user_id].cancel()\n",
        "\n",
        "    # Timer Debounce\n",
        "    async def timer_task():\n",
        "      await asyncio.sleep(TIMER_DEBOUNCE)\n",
        "      full_text = f\" {user_messages[user_id]}\"\n",
        "\n",
        "      del user_messages[user_id]\n",
        "      del user_timers[user_id]\n",
        "\n",
        "      data[\"full_text\"] = full_text\n",
        "\n",
        "      await handler(event, data)\n",
        "\n",
        "    user_timers[user_id] = asyncio.create_task(timer_task())\n",
        "\n",
        "\n",
        "dp.message.middleware(MainMiddleware())\n",
        "\n",
        "# Template for voice or text message\n",
        "async def new_message_text_or_voice(message: types.Message, user_id: int, temp_message: types.Message, response_text: str, last_user_text_: str):\n",
        "    if voice_mode[user_id]:\n",
        "      async with tts_generation_lock:\n",
        "          audio_path = await asyncio.to_thread(voice_engine.text_to_audio, response_text)\n",
        "          if audio_path and os.path.exists(audio_path):\n",
        "              try:\n",
        "                  voice_file = FSInputFile(audio_path)\n",
        "                  await temp_message.delete()\n",
        "                  last_message_obj[user_id] = await message.answer_voice(voice_file)\n",
        "                  os.remove(audio_path)\n",
        "              except Exception as e:\n",
        "                  await temp_message.delete()\n",
        "                  last_message_obj[user_id] = await message.answer(f\"{response_text}\\n(Voice send error: {e})\")\n",
        "          else:\n",
        "              await temp_message.delete()\n",
        "              last_message_obj[user_id] = await message.answer(f\"{response_text}\\n(Voice generation failed)\")\n",
        "    else:\n",
        "        await temp_message.delete()\n",
        "        last_message_obj[user_id] = await message.answer(text=response_text)\n",
        "\n",
        "    last_user_text[user_id] = last_user_text_\n",
        "\n",
        "# Command: /start\n",
        "@dp.message(Command(\"start\"))\n",
        "async def start_command(message: types.Message):\n",
        "    await message.answer(text=WELLCOME_MESSAGE)\n",
        "    try: await message.delete()\n",
        "    except: pass\n",
        "\n",
        "# Command: /reset_memory\n",
        "@dp.message(Command(\"reset_memory\"))\n",
        "async def memory_reset(message: types.Message):\n",
        "    user_id = message.from_user.id\n",
        "    if user_id in user_histories:\n",
        "        user_histories[user_id].clear()\n",
        "    await message.answer(text=CLEANING_MESSAGE)\n",
        "\n",
        "# Command: /reset (Regenerates last message)\n",
        "@dp.message(Command(\"reset\"))\n",
        "async def reset_last_message(message: types.Message):\n",
        "    user_id = message.from_user.id\n",
        "\n",
        "    if last_message_obj[user_id] is None:\n",
        "        await message.answer(text=THERE_ARE_NO_MESSAGE)\n",
        "        return\n",
        "\n",
        "    await last_message_obj[user_id].delete()\n",
        "\n",
        "    if user_id not in voice_mode:\n",
        "      voice_mode[user_id] = False\n",
        "\n",
        "    status_text = VOICE_TEMP_MESSAGE if voice_mode[user_id] else TEXT_TEMP_MESSAGE\n",
        "    temp_message = await message.answer(text=status_text)\n",
        "\n",
        "    try: await message.delete()\n",
        "    except: pass\n",
        "\n",
        "    history_text = get_safe_history(user_id, last_user_text[user_id])\n",
        "\n",
        "    system_instruction = (\n",
        "        f\"You are {BOT_NAME}. {SYSTEM_INSTRUCTION} \"\n",
        "        f\"Here is the chat history:\\n{history_text}\\n\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        async with llm_generation_lock:\n",
        "          new_response_text = await asyncio.to_thread(generate_message, last_user_text[user_id], system_instruction)\n",
        "    except Exception as e:\n",
        "        new_response_text = f\"Error: {e}\"\n",
        "\n",
        "    await new_message_text_or_voice(message, user_id, temp_message, new_response_text, last_user_text[user_id])\n",
        "\n",
        "    if user_id in user_histories and len(user_histories[user_id]) > 0:\n",
        "        user_histories[user_id].pop()\n",
        "        user_histories[user_id].append((\"AI\", new_response_text))\n",
        "\n",
        "# Command: /voice_mode\n",
        "@dp.message(Command(\"voice_mode\"))\n",
        "async def turn_voice_mode(message: types.Message):\n",
        "\n",
        "    user_id = message.from_user.id\n",
        "\n",
        "    if user_id not in voice_mode:\n",
        "      voice_mode[user_id] = False\n",
        "\n",
        "    voice_mode[user_id] = not voice_mode[user_id]\n",
        "\n",
        "    if voice_mode[user_id]:\n",
        "        await message.answer(text=ON_VOICE_MODE_MESSAGE)\n",
        "    else:\n",
        "        await message.answer(text=OFF_VOICE_MODE_MESSAGE )\n",
        "\n",
        "# Main Handler\n",
        "@dp.message()\n",
        "async def base_handler(message: types.Message, full_text: str):\n",
        "    user_id = message.from_user.id\n",
        "\n",
        "    if user_id not in voice_mode:\n",
        "      voice_mode[user_id] = False\n",
        "\n",
        "    status_text = VOICE_TEMP_MESSAGE if voice_mode[user_id] else TEXT_TEMP_MESSAGE\n",
        "    temp_message = await message.answer(text=status_text)\n",
        "\n",
        "    history_text = get_safe_history(user_id, full_text)\n",
        "\n",
        "    system_instruction = (\n",
        "        f\"You are {BOT_NAME}. {SYSTEM_INSTRUCTION} \"\n",
        "        f\"Here is the chat history:\\n{history_text}\\n\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        async with llm_generation_lock:\n",
        "          response_text = await asyncio.to_thread(generate_message, full_text, system_instruction)\n",
        "    except Exception as e:\n",
        "        response_text = f\"Error: {e}\"\n",
        "\n",
        "    user_histories[user_id].append((\"User\", full_text))\n",
        "    user_histories[user_id].append((\"AI\", response_text))\n",
        "\n",
        "    await new_message_text_or_voice(message, user_id, temp_message, response_text, full_text)\n",
        "\n",
        "\n",
        "async def main():\n",
        "    print(\"Bot is running!\")\n",
        "\n",
        "    commands = [\n",
        "        BotCommand(command=\"start\", description=START_DESCRIPTION),\n",
        "        BotCommand(command=\"reset\", description=RESET_DESCRIPTION),\n",
        "        BotCommand(command=\"reset_memory\", description=RESET_MEMORY_DESCRIPTION),\n",
        "        BotCommand(command=\"voice_mode\", description=VOICE_MODE_DESCRIPTION)\n",
        "    ]\n",
        "\n",
        "    await bot.set_my_commands(commands, scope=BotCommandScopeDefault())\n",
        "    await bot.delete_webhook(drop_pending_updates=True)\n",
        "    await dp.start_polling(bot)\n",
        "\n",
        "try:\n",
        "    await main()\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Bot stopped.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uuElLN_kczON",
        "outputId": "efc3d5cd-c3de-4fe2-9073-cda05a23e2a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot is running!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:aiogram.dispatcher:Received SIGINT signal\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "widgets": {
    "application/vnd.jupyter.widget-state+json": {
      "state": {}
    }
  }
}
